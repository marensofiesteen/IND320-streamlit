{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce13f18",
   "metadata": {},
   "source": [
    "# IND320 – Project part 4\n",
    "\n",
    "## 1. Links\n",
    "## 2. AI log\n",
    "## 3. Work log\n",
    "## 4. Elhub production data 2022–2024\n",
    "## 5. Elhub consumption data 2021–2024\n",
    "## 6. Storage in Cassandra and MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b58168",
   "metadata": {},
   "source": [
    "## 1. Linker\n",
    "Github: https://github.com/marensofiesteen/IND320-streamlit/tree/main\n",
    "Streamlit: might come later\n",
    "\n",
    "## 2. AI-log\n",
    "Throughout the project, I used ChatGPT as a support tool for troubleshooting, code review, and improving structure and readability. It helped me debug Streamlit errors, and write cleaner, more consistent Python code. I also used it to refine explanations, write short documentation, and ensure that the application met the assignment requirements. However, ChatGPT had clear limitations - it could not fully understand my specific file structure, debug live code execution, or make design decisions for me, so I had to test, adjust, and reason through several issues on my own.\n",
    "\n",
    "## 3. Work Log\n",
    "In this assignment I first retrieved hourly production and consumption data from the Elhub API for the years 2021–2024. I generated monthly date intervals in the Norwegian time zone and queried the PRODUCTION_PER_GROUP_MBA_HOUR and CONSUMPTION_PER_GROUP_MBA_HOUR endpoints. The responses were cleaned and transformed into a consistent structure using the same processing steps as in earlier parts of the project. I then created new tables in Cassandra and loaded all monthly datasets via Spark, before exporting the complete results to MongoDB Atlas. Old example collections were removed to ensure that only the updated 2021–2024 data remained available for the Streamlit application.\n",
    "\n",
    "For the Streamlit app, I refactored data loading, added caching (@st.cache_data and @st.cache_resource), and implemented clear error handling for missing API connections, empty datasets and invalid selections. I added spinners and progress indicators so that long operations (API calls, model fitting) provide visible feedback. I developed new pages for sliding-window correlation between weather and energy, with selectable lag, window size, price area, group, year and meteorological variable. I also implemented a full SARIMAX forecasting interface with selectable model parameters, training period, forecast horizon, and optional weather-based exogenous variables. All results are visualised with interactive Plotly charts.\n",
    "\n",
    "As part of the optional tasks, I completed several bonus components:\n",
    "(1) Waiting time – spinners and progress bars across the app,\n",
    "(2) Error handling – consistent checks and user-friendly warnings,\n",
    "(3) Map page – a combined map showing NVE Elspot price areas coloured by mean GWh and, when zoomed in, municipality borders from Geonorge, with tooltips and stored click-coordinates,\n",
    "(4) Snow drift – both yearly and monthly snow-drift calculations and plots using the coordinates selected on the map page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc03207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/marenssteen/Documents/IND320/Streamlit/IND320-streamlit/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/marenssteen/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/marenssteen/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bab69ac6-82bf-4c52-94be-cc207c3d00ec;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.5.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound org.apache.cassandra#java-driver-core-shaded;4.18.1 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.1 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.apache.cassandra#java-driver-mapper-runtime;4.18.1 in central\n",
      "\tfound org.apache.cassandra#java-driver-query-builder;4.18.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.19 in central\n",
      ":: resolution report :: resolve 223ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.5.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-core-shaded;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-mapper-runtime;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-query-builder;4.18.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.19 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bab69ac6-82bf-4c52-94be-cc207c3d00ec\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/5ms)\n",
      "25/11/22 11:45:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started.\n",
      "Spark version: 3.5.1\n",
      "Packages: com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "EXECUTE_API = True\n",
    "EXECUTE_localDB = True\n",
    "\n",
    "# --- Make sure Spark uses your venv Python ---\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/Users/marenssteen/Documents/IND320/Streamlit/IND320-streamlit/.venv/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/Users/marenssteen/Documents/IND320/Streamlit/IND320-streamlit/.venv/bin/python\"\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "\n",
    "# --- Force Spark to download JAR even if offline cache is empty ---\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"SparkCassandraApp\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\"\n",
    "    )\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "    .config(\"spark.sql.catalog.mycatalog\", \"com.datastax.spark.connector.datasource.CassandraCatalog\")\n",
    "    # ensures driver binds to localhost correctly\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark session started.\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Packages:\", spark.sparkContext.getConf().get(\"spark.jars.packages\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b11e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period_start_end(year: int, month: int):\n",
    "    \"\"\"\n",
    "    Build a full-month time interval in Norwegian local time (Europe/Oslo).\n",
    "    Start: first day of month at 00:00\n",
    "    End:   last day of month at 23:00\n",
    "    Returned datetimes are timezone-aware.\n",
    "    \"\"\"\n",
    "    norway_tz = pytz.timezone(\"Europe/Oslo\")\n",
    "\n",
    "    # First day of this month\n",
    "    start_naive = datetime(year, month, 1, 0, 0, 0)\n",
    "\n",
    "    # First day of next month\n",
    "    if month == 12:\n",
    "        next_month_naive = datetime(year + 1, 1, 1, 0, 0, 0)\n",
    "    else:\n",
    "        next_month_naive = datetime(year, month + 1, 1, 0, 0, 0)\n",
    "\n",
    "    # Localize both to Norwegian time zone\n",
    "    start_local = norway_tz.localize(start_naive)\n",
    "\n",
    "    # One hour before next month starts → last hour of current month\n",
    "    end_local = norway_tz.localize(next_month_naive) - timedelta(hours=1)\n",
    "\n",
    "    return start_local, end_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b630dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parse_production_data(start_dt, end_dt):\n",
    "    \"\"\"\n",
    "    Fetch PRODUCTION_PER_GROUP_MBA_HOUR data from Elhub API for a given time interval,\n",
    "    then parse into a flat python list of dicts.\n",
    "    \"\"\"\n",
    "    url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "\n",
    "    params = {\n",
    "        \"dataset\": \"PRODUCTION_PER_GROUP_MBA_HOUR\",  # Production dataset\n",
    "        \"startDate\": start_dt.isoformat(),           # ISO 8601 with timezone\n",
    "        \"endDate\": end_dt.isoformat()\n",
    "    }\n",
    "\n",
    "    print(f\"Requesting production data: {start_dt} -> {end_dt}\")\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"⚠️ Failed to get data for {start_dt} - {end_dt}: status {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    data_json = response.json()\n",
    "\n",
    "    parsed_data = []\n",
    "    # The JSON has a list under 'data'; each element has attributes including\n",
    "    # 'productionPerGroupMbaHour' which is itself a list\n",
    "    for data in data_json.get(\"data\", []):\n",
    "        for item in data[\"attributes\"][\"productionPerGroupMbaHour\"]:\n",
    "            # Each 'item' is a dict with keys like:\n",
    "            # endTime, lastUpdatedTime, priceArea, productionGroup, quantityKwh, startTime\n",
    "            parsed_data.append(item)\n",
    "\n",
    "    print(f\"  -> Parsed {len(parsed_data)} rows\")\n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7883e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_cassandra_via_spark(data_list,\n",
    "                                 table: str = \"production_data\",\n",
    "                                 keyspace: str = \"elnub\"):\n",
    "    \"\"\"\n",
    "    Write a list of Elhub production records to a Cassandra table using Spark.\n",
    "    Uses lower-case, snake-like column names as in Assignment 2.\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        print(\"No data to write to Cassandra for this chunk.\")\n",
    "        return\n",
    "\n",
    "    # Convert to pandas DataFrame first\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Force consistent, lower-case column names (same order as in Assignment 2)\n",
    "    df.columns = [\n",
    "        \"endtime\",\n",
    "        \"lastupdatedtime\",\n",
    "        \"pricearea\",\n",
    "        \"productiongroup\",\n",
    "        \"quantitykwh\",\n",
    "        \"starttime\",\n",
    "    ]\n",
    "\n",
    "    print(f\"Writing {len(df)} rows to Cassandra table '{table}' in keyspace '{keyspace}' ...\")\n",
    "\n",
    "    # Use Spark to write into Cassandra (append mode)\n",
    "    (spark.createDataFrame(df)\n",
    "          .write\n",
    "          .format(\"org.apache.spark.sql.cassandra\")\n",
    "          .options(table=table, keyspace=keyspace)\n",
    "          .mode(\"append\")\n",
    "          .save())\n",
    "\n",
    "    print(\"✅ Chunk written to Cassandra.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd12cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Year 2022: starting Elhub import\n",
      "============================\n",
      "\n",
      "\n",
      "--- Month 1 ---\n",
      "Requesting production data: 2022-01-01 00:00:00+01:00 -> 2022-01-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 2 ---\n",
      "Requesting production data: 2022-02-01 00:00:00+01:00 -> 2022-02-28 23:00:00+01:00\n",
      "  -> Parsed 16775 rows\n",
      "Writing 16775 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 3 ---\n",
      "Requesting production data: 2022-03-01 00:00:00+01:00 -> 2022-03-31 23:00:00+02:00\n",
      "  -> Parsed 18550 rows\n",
      "Writing 18550 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 4 ---\n",
      "Requesting production data: 2022-04-01 00:00:00+02:00 -> 2022-04-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 5 ---\n",
      "Requesting production data: 2022-05-01 00:00:00+02:00 -> 2022-05-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 6 ---\n",
      "Requesting production data: 2022-06-01 00:00:00+02:00 -> 2022-06-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 7 ---\n",
      "Requesting production data: 2022-07-01 00:00:00+02:00 -> 2022-07-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 8 ---\n",
      "Requesting production data: 2022-08-01 00:00:00+02:00 -> 2022-08-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 9 ---\n",
      "Requesting production data: 2022-09-01 00:00:00+02:00 -> 2022-09-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 10 ---\n",
      "Requesting production data: 2022-10-01 00:00:00+02:00 -> 2022-10-30 23:00:00+01:00\n",
      "⚠️ Failed to get data for 2022-10-01 00:00:00+02:00 - 2022-10-30 23:00:00+01:00: status 400\n",
      "No data to write to Cassandra for this chunk.\n",
      "Requesting production data: 2022-10-31 00:00:00+01:00 -> 2022-10-31 23:00:00+01:00\n",
      "  -> Parsed 575 rows\n",
      "Writing 575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 11 ---\n",
      "Requesting production data: 2022-11-01 00:00:00+01:00 -> 2022-11-30 23:00:00+01:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 12 ---\n",
      "Requesting production data: 2022-12-01 00:00:00+01:00 -> 2022-12-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "✅ Finished year 2022.\n",
      "\n",
      "\n",
      "============================\n",
      "Year 2023: starting Elhub import\n",
      "============================\n",
      "\n",
      "\n",
      "--- Month 1 ---\n",
      "Requesting production data: 2023-01-01 00:00:00+01:00 -> 2023-01-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 2 ---\n",
      "Requesting production data: 2023-02-01 00:00:00+01:00 -> 2023-02-28 23:00:00+01:00\n",
      "  -> Parsed 16775 rows\n",
      "Writing 16775 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 3 ---\n",
      "Requesting production data: 2023-03-01 00:00:00+01:00 -> 2023-03-31 23:00:00+02:00\n",
      "  -> Parsed 18550 rows\n",
      "Writing 18550 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 4 ---\n",
      "Requesting production data: 2023-04-01 00:00:00+02:00 -> 2023-04-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 5 ---\n",
      "Requesting production data: 2023-05-01 00:00:00+02:00 -> 2023-05-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 6 ---\n",
      "Requesting production data: 2023-06-01 00:00:00+02:00 -> 2023-06-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 7 ---\n",
      "Requesting production data: 2023-07-01 00:00:00+02:00 -> 2023-07-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 8 ---\n",
      "Requesting production data: 2023-08-01 00:00:00+02:00 -> 2023-08-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 9 ---\n",
      "Requesting production data: 2023-09-01 00:00:00+02:00 -> 2023-09-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 10 ---\n",
      "Requesting production data: 2023-10-01 00:00:00+02:00 -> 2023-10-30 23:00:00+01:00\n",
      "⚠️ Failed to get data for 2023-10-01 00:00:00+02:00 - 2023-10-30 23:00:00+01:00: status 400\n",
      "No data to write to Cassandra for this chunk.\n",
      "Requesting production data: 2023-10-31 00:00:00+01:00 -> 2023-10-31 23:00:00+01:00\n",
      "  -> Parsed 575 rows\n",
      "Writing 575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 11 ---\n",
      "Requesting production data: 2023-11-01 00:00:00+01:00 -> 2023-11-30 23:00:00+01:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 12 ---\n",
      "Requesting production data: 2023-12-01 00:00:00+01:00 -> 2023-12-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "✅ Finished year 2023.\n",
      "\n",
      "\n",
      "============================\n",
      "Year 2024: starting Elhub import\n",
      "============================\n",
      "\n",
      "\n",
      "--- Month 1 ---\n",
      "Requesting production data: 2024-01-01 00:00:00+01:00 -> 2024-01-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 2 ---\n",
      "Requesting production data: 2024-02-01 00:00:00+01:00 -> 2024-02-29 23:00:00+01:00\n",
      "  -> Parsed 17375 rows\n",
      "Writing 17375 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 3 ---\n",
      "Requesting production data: 2024-03-01 00:00:00+01:00 -> 2024-03-31 23:00:00+02:00\n",
      "  -> Parsed 18550 rows\n",
      "Writing 18550 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 4 ---\n",
      "Requesting production data: 2024-04-01 00:00:00+02:00 -> 2024-04-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 5 ---\n",
      "Requesting production data: 2024-05-01 00:00:00+02:00 -> 2024-05-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 6 ---\n",
      "Requesting production data: 2024-06-01 00:00:00+02:00 -> 2024-06-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 7 ---\n",
      "Requesting production data: 2024-07-01 00:00:00+02:00 -> 2024-07-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 8 ---\n",
      "Requesting production data: 2024-08-01 00:00:00+02:00 -> 2024-08-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 9 ---\n",
      "Requesting production data: 2024-09-01 00:00:00+02:00 -> 2024-09-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 10 ---\n",
      "Requesting production data: 2024-10-01 00:00:00+02:00 -> 2024-10-30 23:00:00+01:00\n",
      "⚠️ Failed to get data for 2024-10-01 00:00:00+02:00 - 2024-10-30 23:00:00+01:00: status 400\n",
      "No data to write to Cassandra for this chunk.\n",
      "Requesting production data: 2024-10-31 00:00:00+01:00 -> 2024-10-31 23:00:00+01:00\n",
      "  -> Parsed 575 rows\n",
      "Writing 575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 11 ---\n",
      "Requesting production data: 2024-11-01 00:00:00+01:00 -> 2024-11-30 23:00:00+01:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "--- Month 12 ---\n",
      "Requesting production data: 2024-12-01 00:00:00+01:00 -> 2024-12-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'production_data' in keyspace 'elnub' ...\n",
      "✅ Chunk written to Cassandra.\n",
      "\n",
      "✅ Finished year 2024.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if EXECUTE_API and EXECUTE_localDB:\n",
    "    start_year = 2022\n",
    "    end_year = 2024\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\"Year {year}: starting Elhub import\")\n",
    "        print(f\"============================\\n\")\n",
    "\n",
    "        for month in range(1, 13):\n",
    "            print(f\"\\n--- Month {month} ---\")\n",
    "\n",
    "            # Build default full-month interval\n",
    "            start_dt, end_dt = get_period_start_end(year, month)\n",
    "\n",
    "            # Special handling for October (DST transition) – split in two parts\n",
    "            # This follows the same idea as in Assignment 2 for 2021.\n",
    "            # It is a bit defensive, but keeps the time series clean.\n",
    "            if month == 10:\n",
    "                norway_tz = pytz.timezone(\"Europe/Oslo\")\n",
    "\n",
    "                # End of first part: day before last in October at 23:00\n",
    "                # (so we avoid the DST change night in a single chunk)\n",
    "                end_first_part = norway_tz.localize(datetime(year, 10, 30, 23, 0, 0))\n",
    "\n",
    "                parts = [\n",
    "                    (start_dt, end_first_part),\n",
    "                    (\n",
    "                        norway_tz.localize(datetime(year, 10, 31, 0, 0, 0)),\n",
    "                        norway_tz.localize(datetime(year, 10, 31, 23, 0, 0)),\n",
    "                    ),\n",
    "                ]\n",
    "            else:\n",
    "                parts = [(start_dt, end_dt)]\n",
    "\n",
    "            # For each (possibly split) time interval in this month:\n",
    "            for start_part, end_part in parts:\n",
    "                # 1) Fetch and parse data from Elhub\n",
    "                data_list = load_parse_production_data(start_part, end_part)\n",
    "\n",
    "                # 2) Write chunk to Cassandra\n",
    "                write_to_cassandra_via_spark(\n",
    "                    data_list,\n",
    "                    table=\"production_data\",   # Same table as in Assignment 2\n",
    "                    keyspace=\"elnub\",\n",
    "                )\n",
    "\n",
    "        print(f\"\\n✅ Finished year {year}.\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"EXECUTE_API or EXECUTE_localDB is False → skipping Elhub import.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd99b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in production_data: 817978\n",
      "+---------+---------------+-------------------+-------------------+-------------------+-----------+\n",
      "|pricearea|productiongroup|          starttime|            endtime|    lastupdatedtime|quantitykwh|\n",
      "+---------+---------------+-------------------+-------------------+-------------------+-----------+\n",
      "|      NO4|          hydro|2021-01-01 00:00:00|2021-01-01 01:00:00|2024-12-20 10:35:40|  3740830.0|\n",
      "|      NO4|          hydro|2021-01-01 01:00:00|2021-01-01 02:00:00|2024-12-20 10:35:40|  3746663.5|\n",
      "|      NO4|          hydro|2021-01-01 02:00:00|2021-01-01 03:00:00|2024-12-20 10:35:40|  3712439.8|\n",
      "|      NO4|          hydro|2021-01-01 03:00:00|2021-01-01 04:00:00|2024-12-20 10:35:40|  3699229.0|\n",
      "|      NO4|          hydro|2021-01-01 04:00:00|2021-01-01 05:00:00|2024-12-20 10:35:40|  3685393.8|\n",
      "+---------+---------------+-------------------+-------------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell reads the full 'production_data' table from Cassandra\n",
    "# and prints the total number of rows + shows the first 5 records.\n",
    "\n",
    "# Read back from Cassandra to verify that data has been written correctly\n",
    "df_check = (spark.read\n",
    "            .format(\"org.apache.spark.sql.cassandra\")\n",
    "            .options(table=\"production_data\", keyspace=\"elnub\")\n",
    "            .load())\n",
    "\n",
    "print(\"Number of rows in production_data:\", df_check.count())\n",
    "df_check.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d81a4e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded MongoDB Atlas URI from secret file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra rows in production_data: 817978\n",
      "MongoDB Atlas 'production_data' collection cleared.\n",
      "Exporting production in ~82 batches of 10000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 132:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10000/817978\n",
      "Wrote 20000/817978\n",
      "Wrote 30000/817978\n",
      "Wrote 40000/817978\n",
      "Wrote 50000/817978\n",
      "Wrote 60000/817978\n",
      "Wrote 70000/817978\n",
      "Wrote 80000/817978\n",
      "Wrote 90000/817978\n",
      "Wrote 100000/817978\n",
      "Wrote 110000/817978\n",
      "Wrote 120000/817978\n",
      "Wrote 130000/817978\n",
      "Wrote 140000/817978\n",
      "Wrote 150000/817978\n",
      "Wrote 160000/817978\n",
      "Wrote 170000/817978\n",
      "Wrote 180000/817978\n",
      "Wrote 190000/817978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 200000/817978\n",
      "Wrote 210000/817978\n",
      "Wrote 220000/817978\n",
      "Wrote 230000/817978\n",
      "Wrote 240000/817978\n",
      "Wrote 250000/817978\n",
      "Wrote 260000/817978\n",
      "Wrote 270000/817978\n",
      "Wrote 280000/817978\n",
      "Wrote 290000/817978\n",
      "Wrote 300000/817978\n",
      "Wrote 310000/817978\n",
      "Wrote 320000/817978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 138:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 330000/817978\n",
      "Wrote 340000/817978\n",
      "Wrote 350000/817978\n",
      "Wrote 360000/817978\n",
      "Wrote 370000/817978\n",
      "Wrote 380000/817978\n",
      "Wrote 390000/817978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 139:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 400000/817978\n",
      "Wrote 410000/817978\n",
      "Wrote 420000/817978\n",
      "Wrote 430000/817978\n",
      "Wrote 440000/817978\n",
      "Wrote 450000/817978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 142:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 460000/817978\n",
      "Wrote 470000/817978\n",
      "Wrote 480000/817978\n",
      "Wrote 490000/817978\n",
      "Wrote 500000/817978\n",
      "Wrote 510000/817978\n",
      "Wrote 520000/817978\n",
      "Wrote 530000/817978\n",
      "Wrote 540000/817978\n",
      "Wrote 550000/817978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 144:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 560000/817978\n",
      "Wrote 570000/817978\n",
      "Wrote 580000/817978\n",
      "Wrote 590000/817978\n",
      "Wrote 600000/817978\n",
      "Wrote 610000/817978\n",
      "Wrote 620000/817978\n",
      "Wrote 630000/817978\n",
      "Wrote 640000/817978\n",
      "Wrote 650000/817978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 145:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 660000/817978\n",
      "Wrote 670000/817978\n",
      "Wrote 680000/817978\n",
      "Wrote 690000/817978\n",
      "Wrote 700000/817978\n",
      "Wrote 710000/817978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 147:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 720000/817978\n",
      "Wrote 730000/817978\n",
      "Wrote 740000/817978\n",
      "Wrote 750000/817978\n",
      "Wrote 760000/817978\n",
      "Wrote 770000/817978\n",
      "Wrote 780000/817978\n",
      "Wrote 790000/817978\n",
      "Wrote 800000/817978\n",
      "Wrote 810000/817978\n",
      "✅ Export to Atlas completed (production).\n",
      "Atlas count: 817978\n"
     ]
    }
   ],
   "source": [
    "### Exporting data from Cassandra to MongoDB Atlas (updated for Part 4)\n",
    "\n",
    "\"\"\"In Assignment 2, data was exported from Cassandra to a **local MongoDB instance**\n",
    "(`mongodb://localhost:27017`).  \n",
    "In Assignment 4, the Streamlit application uses **MongoDB Atlas**, so the export\n",
    "pipeline is updated accordingly:\n",
    "\n",
    "- No local MongoDB is used anymore.\n",
    "- Atlas credentials are stored securely in `.secrets_notebook.toml` (ignored by Git).\n",
    "- Both *production_data* and *consumption_data* are now exported directly to Atlas.\n",
    "\n",
    "The code below replaces the original local-Mongo versions.\"\"\"\n",
    "\n",
    "\n",
    "# --- Export production_data (2021-2024) from Cassandra to MongoDB Atlas ---\n",
    "\n",
    "import tomllib\n",
    "from pymongo import MongoClient\n",
    "import math\n",
    "\n",
    "# Load URIs securely\n",
    "with open(\".secrets_notebook.toml\", \"rb\") as f:\n",
    "    secrets = tomllib.load(f)\n",
    "\n",
    "mongo_uri_atlas = secrets[\"mongo\"][\"uri\"]\n",
    "db_name_atlas = secrets[\"mongo\"][\"db\"]\n",
    "\n",
    "print(\"✅ Loaded MongoDB Atlas URI from secret file.\")\n",
    "\n",
    "# 0) Read production data from Cassandra\n",
    "df_prod_atlas = (\n",
    "    spark.read\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(table=\"production_data\", keyspace=\"elnub\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "total_prod = df_prod_atlas.count()\n",
    "print(\"Cassandra rows in production_data:\", total_prod)\n",
    "\n",
    "if total_prod == 0:\n",
    "    print(\"⚠️ No rows found in production_data. Nothing to export.\")\n",
    "else:\n",
    "    # 1) Connect to Atlas\n",
    "    client_atlas = MongoClient(mongo_uri_atlas)\n",
    "    db_atlas = client_atlas[db_name_atlas]\n",
    "    coll_prod_atlas = db_atlas[\"production_data\"]\n",
    "\n",
    "    # Clear existing data\n",
    "    coll_prod_atlas.delete_many({})\n",
    "    print(\"MongoDB Atlas 'production_data' collection cleared.\")\n",
    "\n",
    "    # 2) Export in batches\n",
    "    BATCH = 10_000\n",
    "    num_batches = math.ceil(total_prod / BATCH)\n",
    "    print(f\"Exporting production in ~{num_batches} batches of {BATCH} rows\")\n",
    "\n",
    "    buffer = []\n",
    "    count_written = 0\n",
    "\n",
    "    for row in df_prod_atlas.toLocalIterator():\n",
    "        buffer.append(row.asDict())\n",
    "        if len(buffer) >= BATCH:\n",
    "            coll_prod_atlas.insert_many(buffer)\n",
    "            count_written += len(buffer)\n",
    "            print(f\"Wrote {count_written}/{total_prod}\")\n",
    "            buffer = []\n",
    "\n",
    "    if buffer:\n",
    "        coll_prod_atlas.insert_many(buffer)\n",
    "        count_written += len(buffer)\n",
    "\n",
    "    print(\"✅ Export to Atlas completed (production).\")\n",
    "    print(\"Atlas count:\", coll_prod_atlas.count_documents({}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b81ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cassandra table 'elnub.consumption_data' is ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare Cassandra table for consumption data (2021-2024) ---\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "# Connect to Cassandra cluster\n",
    "cluster = Cluster([\"127.0.0.1\"], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "if EXECUTE_localDB:\n",
    "    # Make sure keyspace exists\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS elnub WITH REPLICATION = {\n",
    "            'class': 'SimpleStrategy',\n",
    "            'replication_factor': 1}\n",
    "    \"\"\")\n",
    "\n",
    "    # Use keyspace\n",
    "    session.set_keyspace(\"elnub\")\n",
    "\n",
    "    # Create consumption_data table if it does not exist\n",
    "    session.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS elnub.consumption_data (\n",
    "            priceArea TEXT,\n",
    "            consumptionGroup TEXT,\n",
    "            startTime TIMESTAMP,\n",
    "            quantityKwh DOUBLE,\n",
    "            endTime TIMESTAMP,\n",
    "            lastUpdatedTime TIMESTAMP,\n",
    "            PRIMARY KEY ((priceArea, consumptionGroup), startTime)\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"✅ Cassandra table 'elnub.consumption_data' is ready.\")\n",
    "else:\n",
    "    print(\"EXECUTE_localDB is False → skipping Cassandra table creation for consumption.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a7e9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parse_consumption_data(start_dt, end_dt):\n",
    "    \"\"\"\n",
    "    Fetch CONSUMPTION_PER_GROUP_MBA_HOUR data from Elhub API for a given time interval,\n",
    "    then parse into a flat python list of dicts.\n",
    "    \"\"\"\n",
    "    url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "\n",
    "    params = {\n",
    "        \"dataset\": \"CONSUMPTION_PER_GROUP_MBA_HOUR\",  # Consumption dataset\n",
    "        \"startDate\": start_dt.isoformat(),            # ISO 8601 with timezone\n",
    "        \"endDate\": end_dt.isoformat()\n",
    "    }\n",
    "\n",
    "    print(f\"Requesting consumption data: {start_dt} -> {end_dt}\")\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"⚠️ Failed to get data for {start_dt} - {end_dt}: status {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    data_json = response.json()\n",
    "\n",
    "    parsed_data = []\n",
    "    # The JSON has a list under 'data'; each element has attributes including\n",
    "    # 'consumptionPerGroupMbaHour' which is itself a list\n",
    "    for data in data_json.get(\"data\", []):\n",
    "        for item in data[\"attributes\"][\"consumptionPerGroupMbaHour\"]:\n",
    "            # Each 'item' is a dict with keys like:\n",
    "            # endTime, lastUpdatedTime, priceArea, consumptionGroup, quantityKwh, startTime\n",
    "            parsed_data.append(item)\n",
    "\n",
    "    print(f\"  -> Parsed {len(parsed_data)} rows\")\n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d88e915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_consumption_to_cassandra_via_spark(data_list,\n",
    "                                             table: str = \"consumption_data\",\n",
    "                                             keyspace: str = \"elnub\"):\n",
    "    \"\"\"\n",
    "    Write a list of Elhub consumption records to a Cassandra table using Spark.\n",
    "    We explicitly select only the columns we need from the JSON.\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        print(\"No consumption data to write to Cassandra for this chunk.\")\n",
    "        return\n",
    "\n",
    "    # Convert to pandas DataFrame first\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Keep only the columns we actually need from the Elhub JSON\n",
    "    # Typical keys are: endTime, lastUpdatedTime, priceArea, consumptionGroup, quantityKwh, startTime\n",
    "    needed_cols = [\n",
    "        \"endTime\",\n",
    "        \"lastUpdatedTime\",\n",
    "        \"priceArea\",\n",
    "        \"consumptionGroup\",\n",
    "        \"quantityKwh\",\n",
    "        \"startTime\",\n",
    "    ]\n",
    "\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"⚠️ Missing expected columns in consumption data:\", missing)\n",
    "        print(\"Available columns:\", list(df.columns))\n",
    "        return\n",
    "\n",
    "    df = df[needed_cols]\n",
    "\n",
    "    # Rename to lower-case names used in Cassandra (Cassandra is case-insensitive, but we stay consistent)\n",
    "    df.columns = [\n",
    "        \"endtime\",\n",
    "        \"lastupdatedtime\",\n",
    "        \"pricearea\",\n",
    "        \"consumptiongroup\",\n",
    "        \"quantitykwh\",\n",
    "        \"starttime\",\n",
    "    ]\n",
    "\n",
    "    print(f\"Writing {len(df)} rows to Cassandra table '{table}' in keyspace '{keyspace}' ...\")\n",
    "\n",
    "    (spark.createDataFrame(df)\n",
    "          .write\n",
    "          .format(\"org.apache.spark.sql.cassandra\")\n",
    "          .options(table=table, keyspace=keyspace)\n",
    "          .mode(\"append\")\n",
    "          .save())\n",
    "\n",
    "    print(\"✅ Consumption chunk written to Cassandra.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de4bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Year 2021: starting Elhub CONSUMPTION import\n",
      "============================\n",
      "\n",
      "\n",
      "--- Month 1 ---\n",
      "Requesting consumption data: 2021-01-01 00:00:00+01:00 -> 2021-01-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 2 ---\n",
      "Requesting consumption data: 2021-02-01 00:00:00+01:00 -> 2021-02-28 23:00:00+01:00\n",
      "  -> Parsed 16775 rows\n",
      "Writing 16775 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 3 ---\n",
      "Requesting consumption data: 2021-03-01 00:00:00+01:00 -> 2021-03-31 23:00:00+02:00\n",
      "  -> Parsed 18550 rows\n",
      "Writing 18550 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 4 ---\n",
      "Requesting consumption data: 2021-04-01 00:00:00+02:00 -> 2021-04-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 5 ---\n",
      "Requesting consumption data: 2021-05-01 00:00:00+02:00 -> 2021-05-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 6 ---\n",
      "Requesting consumption data: 2021-06-01 00:00:00+02:00 -> 2021-06-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 7 ---\n",
      "Requesting consumption data: 2021-07-01 00:00:00+02:00 -> 2021-07-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 8 ---\n",
      "Requesting consumption data: 2021-08-01 00:00:00+02:00 -> 2021-08-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 9 ---\n",
      "Requesting consumption data: 2021-09-01 00:00:00+02:00 -> 2021-09-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 10 ---\n",
      "Requesting consumption data: 2021-10-01 00:00:00+02:00 -> 2021-10-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "Requesting consumption data: 2021-10-31 00:00:00+02:00 -> 2021-10-31 23:00:00+01:00\n",
      "  -> Parsed 600 rows\n",
      "Writing 600 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 11 ---\n",
      "Requesting consumption data: 2021-11-01 00:00:00+01:00 -> 2021-11-30 23:00:00+01:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 12 ---\n",
      "Requesting consumption data: 2021-12-01 00:00:00+01:00 -> 2021-12-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "✅ Finished CONSUMPTION year 2021.\n",
      "\n",
      "\n",
      "============================\n",
      "Year 2022: starting Elhub CONSUMPTION import\n",
      "============================\n",
      "\n",
      "\n",
      "--- Month 1 ---\n",
      "Requesting consumption data: 2022-01-01 00:00:00+01:00 -> 2022-01-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 2 ---\n",
      "Requesting consumption data: 2022-02-01 00:00:00+01:00 -> 2022-02-28 23:00:00+01:00\n",
      "  -> Parsed 16775 rows\n",
      "Writing 16775 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 3 ---\n",
      "Requesting consumption data: 2022-03-01 00:00:00+01:00 -> 2022-03-31 23:00:00+02:00\n",
      "  -> Parsed 18550 rows\n",
      "Writing 18550 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 4 ---\n",
      "Requesting consumption data: 2022-04-01 00:00:00+02:00 -> 2022-04-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 5 ---\n",
      "Requesting consumption data: 2022-05-01 00:00:00+02:00 -> 2022-05-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 6 ---\n",
      "Requesting consumption data: 2022-06-01 00:00:00+02:00 -> 2022-06-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 7 ---\n",
      "Requesting consumption data: 2022-07-01 00:00:00+02:00 -> 2022-07-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 8 ---\n",
      "Requesting consumption data: 2022-08-01 00:00:00+02:00 -> 2022-08-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 9 ---\n",
      "Requesting consumption data: 2022-09-01 00:00:00+02:00 -> 2022-09-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 10 ---\n",
      "Requesting consumption data: 2022-10-01 00:00:00+02:00 -> 2022-10-30 23:00:00+01:00\n",
      "⚠️ Failed to get data for 2022-10-01 00:00:00+02:00 - 2022-10-30 23:00:00+01:00: status 400\n",
      "No consumption data to write to Cassandra for this chunk.\n",
      "Requesting consumption data: 2022-10-31 00:00:00+01:00 -> 2022-10-31 23:00:00+01:00\n",
      "  -> Parsed 575 rows\n",
      "Writing 575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 11 ---\n",
      "Requesting consumption data: 2022-11-01 00:00:00+01:00 -> 2022-11-30 23:00:00+01:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 12 ---\n",
      "Requesting consumption data: 2022-12-01 00:00:00+01:00 -> 2022-12-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "✅ Finished CONSUMPTION year 2022.\n",
      "\n",
      "\n",
      "============================\n",
      "Year 2023: starting Elhub CONSUMPTION import\n",
      "============================\n",
      "\n",
      "\n",
      "--- Month 1 ---\n",
      "Requesting consumption data: 2023-01-01 00:00:00+01:00 -> 2023-01-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 2 ---\n",
      "Requesting consumption data: 2023-02-01 00:00:00+01:00 -> 2023-02-28 23:00:00+01:00\n",
      "  -> Parsed 16775 rows\n",
      "Writing 16775 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 3 ---\n",
      "Requesting consumption data: 2023-03-01 00:00:00+01:00 -> 2023-03-31 23:00:00+02:00\n",
      "  -> Parsed 18550 rows\n",
      "Writing 18550 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 4 ---\n",
      "Requesting consumption data: 2023-04-01 00:00:00+02:00 -> 2023-04-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 5 ---\n",
      "Requesting consumption data: 2023-05-01 00:00:00+02:00 -> 2023-05-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 6 ---\n",
      "Requesting consumption data: 2023-06-01 00:00:00+02:00 -> 2023-06-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 7 ---\n",
      "Requesting consumption data: 2023-07-01 00:00:00+02:00 -> 2023-07-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 8 ---\n",
      "Requesting consumption data: 2023-08-01 00:00:00+02:00 -> 2023-08-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 9 ---\n",
      "Requesting consumption data: 2023-09-01 00:00:00+02:00 -> 2023-09-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 10 ---\n",
      "Requesting consumption data: 2023-10-01 00:00:00+02:00 -> 2023-10-30 23:00:00+01:00\n",
      "⚠️ Failed to get data for 2023-10-01 00:00:00+02:00 - 2023-10-30 23:00:00+01:00: status 400\n",
      "No consumption data to write to Cassandra for this chunk.\n",
      "Requesting consumption data: 2023-10-31 00:00:00+01:00 -> 2023-10-31 23:00:00+01:00\n",
      "  -> Parsed 575 rows\n",
      "Writing 575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 11 ---\n",
      "Requesting consumption data: 2023-11-01 00:00:00+01:00 -> 2023-11-30 23:00:00+01:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 12 ---\n",
      "Requesting consumption data: 2023-12-01 00:00:00+01:00 -> 2023-12-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "✅ Finished CONSUMPTION year 2023.\n",
      "\n",
      "\n",
      "============================\n",
      "Year 2024: starting Elhub CONSUMPTION import\n",
      "============================\n",
      "\n",
      "\n",
      "--- Month 1 ---\n",
      "Requesting consumption data: 2024-01-01 00:00:00+01:00 -> 2024-01-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 2 ---\n",
      "Requesting consumption data: 2024-02-01 00:00:00+01:00 -> 2024-02-29 23:00:00+01:00\n",
      "  -> Parsed 17375 rows\n",
      "Writing 17375 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 3 ---\n",
      "Requesting consumption data: 2024-03-01 00:00:00+01:00 -> 2024-03-31 23:00:00+02:00\n",
      "  -> Parsed 18550 rows\n",
      "Writing 18550 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 4 ---\n",
      "Requesting consumption data: 2024-04-01 00:00:00+02:00 -> 2024-04-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 5 ---\n",
      "Requesting consumption data: 2024-05-01 00:00:00+02:00 -> 2024-05-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 6 ---\n",
      "Requesting consumption data: 2024-06-01 00:00:00+02:00 -> 2024-06-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 7 ---\n",
      "Requesting consumption data: 2024-07-01 00:00:00+02:00 -> 2024-07-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 8 ---\n",
      "Requesting consumption data: 2024-08-01 00:00:00+02:00 -> 2024-08-31 23:00:00+02:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 9 ---\n",
      "Requesting consumption data: 2024-09-01 00:00:00+02:00 -> 2024-09-30 23:00:00+02:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 10 ---\n",
      "Requesting consumption data: 2024-10-01 00:00:00+02:00 -> 2024-10-30 23:00:00+01:00\n",
      "⚠️ Failed to get data for 2024-10-01 00:00:00+02:00 - 2024-10-30 23:00:00+01:00: status 400\n",
      "No consumption data to write to Cassandra for this chunk.\n",
      "Requesting consumption data: 2024-10-31 00:00:00+01:00 -> 2024-10-31 23:00:00+01:00\n",
      "  -> Parsed 575 rows\n",
      "Writing 575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 11 ---\n",
      "Requesting consumption data: 2024-11-01 00:00:00+01:00 -> 2024-11-30 23:00:00+01:00\n",
      "  -> Parsed 17975 rows\n",
      "Writing 17975 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "--- Month 12 ---\n",
      "Requesting consumption data: 2024-12-01 00:00:00+01:00 -> 2024-12-31 23:00:00+01:00\n",
      "  -> Parsed 18575 rows\n",
      "Writing 18575 rows to Cassandra table 'consumption_data' in keyspace 'elnub' ...\n",
      "✅ Consumption chunk written to Cassandra.\n",
      "\n",
      "✅ Finished CONSUMPTION year 2024.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if EXECUTE_API and EXECUTE_localDB:\n",
    "    start_year_cons = 2021\n",
    "    end_year_cons = 2024\n",
    "\n",
    "    for year in range(start_year_cons, end_year_cons + 1):\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\"Year {year}: starting Elhub CONSUMPTION import\")\n",
    "        print(f\"============================\\n\")\n",
    "\n",
    "        for month in range(1, 13):\n",
    "            print(f\"\\n--- Month {month} ---\")\n",
    "\n",
    "            # Build default full-month interval\n",
    "            start_dt, end_dt = get_period_start_end(year, month)\n",
    "\n",
    "            # Same defensive DST handling for October as for production\n",
    "            if month == 10:\n",
    "                norway_tz = pytz.timezone(\"Europe/Oslo\")\n",
    "\n",
    "                end_first_part = norway_tz.localize(datetime(year, 10, 30, 23, 0, 0))\n",
    "\n",
    "                parts = [\n",
    "                    (start_dt, end_first_part),\n",
    "                    (\n",
    "                        norway_tz.localize(datetime(year, 10, 31, 0, 0, 0)),\n",
    "                        norway_tz.localize(datetime(year, 10, 31, 23, 0, 0)),\n",
    "                    ),\n",
    "                ]\n",
    "            else:\n",
    "                parts = [(start_dt, end_dt)]\n",
    "\n",
    "            for start_part, end_part in parts:\n",
    "                # 1) Fetch and parse consumption data from Elhub\n",
    "                cons_list = load_parse_consumption_data(start_part, end_part)\n",
    "\n",
    "                # 2) Write chunk to Cassandra\n",
    "                write_consumption_to_cassandra_via_spark(\n",
    "                    cons_list,\n",
    "                    table=\"consumption_data\",\n",
    "                    keyspace=\"elnub\",\n",
    "                )\n",
    "\n",
    "        print(f\"\\n✅ Finished CONSUMPTION year {year}.\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"EXECUTE_API or EXECUTE_localDB is False → skipping Elhub CONSUMPTION import.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d226c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded MongoDB Atlas URI from secret file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra rows in consumption_data: 821300\n",
      "MongoDB Atlas 'consumption_data' collection cleared.\n",
      "Exporting consumption in ~83 batches of 10000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 170:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10000/821300\n",
      "Wrote 20000/821300\n",
      "Wrote 30000/821300\n",
      "Wrote 40000/821300\n",
      "Wrote 50000/821300\n",
      "Wrote 60000/821300\n",
      "Wrote 70000/821300\n",
      "Wrote 80000/821300\n",
      "Wrote 90000/821300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 172:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 100000/821300\n",
      "Wrote 110000/821300\n",
      "Wrote 120000/821300\n",
      "Wrote 130000/821300\n",
      "Wrote 140000/821300\n",
      "Wrote 150000/821300\n",
      "Wrote 160000/821300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 173:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 170000/821300\n",
      "Wrote 180000/821300\n",
      "Wrote 190000/821300\n",
      "Wrote 200000/821300\n",
      "Wrote 210000/821300\n",
      "Wrote 220000/821300\n",
      "Wrote 230000/821300\n",
      "Wrote 240000/821300\n",
      "Wrote 250000/821300\n",
      "Wrote 260000/821300\n",
      "Wrote 270000/821300\n",
      "Wrote 280000/821300\n",
      "Wrote 290000/821300\n",
      "Wrote 300000/821300\n",
      "Wrote 310000/821300\n",
      "Wrote 320000/821300\n",
      "Wrote 330000/821300\n",
      "Wrote 340000/821300\n",
      "Wrote 350000/821300\n",
      "Wrote 360000/821300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 177:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 370000/821300\n",
      "Wrote 380000/821300\n",
      "Wrote 390000/821300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 178:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 400000/821300\n",
      "Wrote 410000/821300\n",
      "Wrote 420000/821300\n",
      "Wrote 430000/821300\n",
      "Wrote 440000/821300\n",
      "Wrote 450000/821300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 180:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 460000/821300\n",
      "Wrote 470000/821300\n",
      "Wrote 480000/821300\n",
      "Wrote 490000/821300\n",
      "Wrote 500000/821300\n",
      "Wrote 510000/821300\n",
      "Wrote 520000/821300\n",
      "Wrote 530000/821300\n",
      "Wrote 540000/821300\n",
      "Wrote 550000/821300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 181:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 560000/821300\n",
      "Wrote 570000/821300\n",
      "Wrote 580000/821300\n",
      "Wrote 590000/821300\n",
      "Wrote 600000/821300\n",
      "Wrote 610000/821300\n",
      "Wrote 620000/821300\n",
      "Wrote 630000/821300\n",
      "Wrote 640000/821300\n",
      "Wrote 650000/821300\n",
      "Wrote 660000/821300\n",
      "Wrote 670000/821300\n",
      "Wrote 680000/821300\n",
      "Wrote 690000/821300\n",
      "Wrote 700000/821300\n",
      "Wrote 710000/821300\n",
      "Wrote 720000/821300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 186:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 730000/821300\n",
      "Wrote 740000/821300\n",
      "Wrote 750000/821300\n",
      "Wrote 760000/821300\n",
      "Wrote 770000/821300\n",
      "Wrote 780000/821300\n",
      "Wrote 790000/821300\n",
      "Wrote 800000/821300\n",
      "Wrote 810000/821300\n",
      "Wrote 820000/821300\n",
      "✅ Export to Atlas completed (consumption).\n",
      "Atlas count: 821300\n"
     ]
    }
   ],
   "source": [
    "### Exporting data from Cassandra to MongoDB Atlas (updated for Part 4)\n",
    "\n",
    "\"\"\"In Assignment 2, data was exported from Cassandra to a **local MongoDB instance**\n",
    "(`mongodb://localhost:27017`).  \n",
    "In Assignment 4, the Streamlit application uses **MongoDB Atlas**, so the export\n",
    "pipeline is updated accordingly:\n",
    "\n",
    "- No local MongoDB is used anymore.\n",
    "- Atlas credentials are stored securely in `.secrets_notebook.toml` (ignored by Git).\n",
    "- Both *production_data* and *consumption_data* are now exported directly to Atlas.\n",
    "\n",
    "The code below replaces the original local-Mongo versions.\"\"\"\n",
    "\n",
    "# --- Export consumption_data (2021-2024) from Cassandra to MongoDB Atlas ---\n",
    "\n",
    "import tomllib\n",
    "from pymongo import MongoClient\n",
    "import math\n",
    "\n",
    "# Load URIs securely\n",
    "with open(\".secrets_notebook.toml\", \"rb\") as f:\n",
    "    secrets = tomllib.load(f)\n",
    "\n",
    "mongo_uri_atlas = secrets[\"mongo\"][\"uri\"]\n",
    "db_name_atlas = secrets[\"mongo\"][\"db\"]\n",
    "\n",
    "print(\"✅ Loaded MongoDB Atlas URI from secret file.\")\n",
    "\n",
    "# 0) Read consumption data from Cassandra\n",
    "df_cons_atlas = (\n",
    "    spark.read\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(table=\"consumption_data\", keyspace=\"elnub\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "total_cons = df_cons_atlas.count()\n",
    "print(\"Cassandra rows in consumption_data:\", total_cons)\n",
    "\n",
    "if total_cons == 0:\n",
    "    print(\"⚠️ No rows found in consumption_data. Nothing to export.\")\n",
    "else:\n",
    "    # 1) Connect to Atlas\n",
    "    client_atlas = MongoClient(mongo_uri_atlas)\n",
    "    db_atlas = client_atlas[db_name_atlas]\n",
    "    coll_cons_atlas = db_atlas[\"consumption_data\"]\n",
    "\n",
    "    # Clear existing data\n",
    "    coll_cons_atlas.delete_many({})\n",
    "    print(\"MongoDB Atlas 'consumption_data' collection cleared.\")\n",
    "\n",
    "    # 2) Export in batches\n",
    "    BATCH = 10_000\n",
    "    num_batches = math.ceil(total_cons / BATCH)\n",
    "    print(f\"Exporting consumption in ~{num_batches} batches of {BATCH} rows\")\n",
    "\n",
    "    buffer = []\n",
    "    count_written = 0\n",
    "\n",
    "    for row in df_cons_atlas.toLocalIterator():\n",
    "        buffer.append(row.asDict())\n",
    "        if len(buffer) >= BATCH:\n",
    "            coll_cons_atlas.insert_many(buffer)\n",
    "            count_written += len(buffer)\n",
    "            print(f\"Wrote {count_written}/{total_cons}\")\n",
    "            buffer = []\n",
    "\n",
    "    if buffer:\n",
    "        coll_cons_atlas.insert_many(buffer)\n",
    "        count_written += len(buffer)\n",
    "\n",
    "    print(\"✅ Export to Atlas completed (consumption).\")\n",
    "    print(\"Atlas count:\", coll_cons_atlas.count_documents({}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IND320 .venv)",
   "language": "python",
   "name": "ind320"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
